# -*- coding: utf-8 -*-
"""QAISU MODEL 1 - PYTHON CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G429Hwi2QfXO867ptjpfH3rmkKKeDgLq
"""

"""
Complete system with:
- ML pipeline (RF, XGB, Logistic Regression)
- Comprehensive EDA
- Feature Importance Analysis
- QA metrics and risk scoring
- Cost of Poor Quality Trade-off (Model 1)
- Cost Center Balancing optimization (Model 2)
- Batch Flow / Inter-Operation Transition optimization (Model 3)
- Anomaly/Inefficiency Informed Assignment Optimization (Model 4)
- HYBRID OPTIMIZATION DECISION (Model 1-4 entegrasyonu)
- Complete model performance tracking
- Confusion matrix analysis
- All outputs in single CSV (Superset entegrasyonu için hazır)
"""

import pandas as pd
import numpy as np
import json
import logging
import re
from typing import Optional, Dict, Any, List
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_recall_fscore_support, f1_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.feature_selection import mutual_info_classif
from sklearn.cluster import KMeans

try:
    from xgboost import XGBClassifier
except Exception:
    XGBClassifier = None

import pulp
import warnings
warnings.filterwarnings('ignore')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QualityDecisionSystem:
    def __init__(self, filepath: Optional[str] = None, config: Optional[Dict[str, Any]] = None):
        self.filepath = filepath
        self.df_raw = None
        self.df_model = None
        self.label_encoders = {}
        self.rf_model = None
        self.xgb_model = None
        self.logreg_model = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = None
        self.feature_cols = None
        self.results_df = None
        self.config = config or {}
        self.random_state = self.config.get('random_state', 42)

        self.eda_summary = {}
        self.model_metrics = {}
        self.feature_importance = None
        self.qa_metrics_summary = None
        self.confusion_matrices = {}

    # Robust Label Normalization Method
    def _normalize_label(self, x: Any) -> str:
        """Normalize DECISION_TYPE label to a canonical string form (e.g., 'Karar-X')."""
        if pd.isna(x):
            return '<<UNLABELED>>'
        s = str(x).strip()

        if s.lower().startswith('karar'):
            m = re.search(r'(\d+)', s)
            if m:
                return 'Karar-' + m.group(1)
            return s

        if s.isdigit():
            return 'Karar-' + s

        if s == '<<UNLABELED>>' or s == '-1' or s == '':
            return '<<UNLABELED>>'

        return s

    # Data load & basic prep
    def load_and_prep_data(self):
        if self.filepath is None:
            raise ValueError("No filepath provided.")
        try:
            df = pd.read_excel(self.filepath)
        except FileNotFoundError:
            logger.error(f"File not found: {self.filepath}. Creating dummy data for demonstration.")
            df = self._create_dummy_data()

        for col in ['FLOW_END_DATE', 'YARATILMA_TARIHI']:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
        column_mapping = {
            'PROSES_TIPI': 'PROCESS_TYPE',
            'NUMERIK_SONUC': 'INSPECTION_RESULT',
            'UST_LIMIT': 'UPPER_LIMIT',
            'ALT_LIMIT': 'LOWER_LIMIT',
            'HATA_TURU': 'DEFECT_TYPE',
            'Grup Bşk.': 'RESPONSIBLE_UNIT',
            'IS_EMRI_TAMAMLANMA_STOK_YERI': 'STOCK_PLACE',
            'IS_EMRI': 'WORK_ORDER',
            'KSYM_SORUMLUSU_KARAR': 'DECISION_TYPE',
            'NIHAI_KARAR': 'ACTUAL_DECISION',
            'MM': 'MM',
            'NUMUNE_MIKTARI': 'NUMUNE_MIKTARI',
            'MUAYENE_TIPI': 'MUAYENE_TIPI',
            'OPERASYON_NO': 'OPERASYON_NO',
            'FLOW_END_DATE': 'FLOW_END_DATE',
            'YARATILMA_TARIHI': 'YARATILMA_TARIHI',
        }
        exist_map = {k: v for k, v in column_mapping.items() if k in df.columns}
        if exist_map:
            df = df.rename(columns=exist_map)
        for col in df.columns:
            if df[col].dtype == 'O' or df[col].dtype.name == 'category':
                if df[col].isnull().any():
                    try:
                        df[col] = df[col].fillna(df[col].mode()[0])
                    except Exception:
                        df[col] = df[col].fillna('UNKNOWN')
            else:
                if df[col].isnull().any():
                    try:
                        df[col] = df[col].fillna(df[col].median())
                    except Exception:
                        df[col] = df[col].fillna(0)

        if all(c in df.columns for c in ['FLOW_END_DATE', 'YARATILMA_TARIHI']):
            df['PROCESS_TIME'] = (df['FLOW_END_DATE'] - df['YARATILMA_TARIHI']).dt.total_seconds().fillna(0)
        else:
            df['PROCESS_TIME'] = 0.0

        self.df_raw = df
        logger.info("Loaded data shape: %s", df.shape)
        return self

    # DUMMY DATA CREATOR (for demo/file missing)
    def _create_dummy_data(self, n_rows=100):
        data = {
            'PROSES_TIPI': np.random.choice(['PROC_A', 'PROC_B'], n_rows),
            'NUMERIK_SONUC': np.random.rand(n_rows) * 100,
            'UST_LIMIT': np.full(n_rows, 80.0),
            'ALT_LIMIT': np.full(n_rows, 20.0),
            'HATA_TURU': np.random.choice(['DEF_1', 'DEF_2'], n_rows),
            'Grup Bşk.': np.random.choice(['UNIT_X', 'UNIT_Y'], n_rows),
            'IS_EMRI_TAMAMLANMA_STOK_YERI': np.random.choice(['S1', 'S2'], n_rows),
            'IS_EMRI': [f'WO-{i+1:03d}' for i in range(n_rows)],
            'KSYM_SORUMLUSU_KARAR': np.random.choice(['Karar-1', 'Karar-2', 'Karar-3', np.nan], n_rows, p=[0.4, 0.3, 0.2, 0.1]),
            'NIHAI_KARAR': np.random.choice(['Karar-1', 'Karar-2', 'Karar-3', np.nan], n_rows, p=[0.5, 0.3, 0.1, 0.1]),
            'MM': np.random.choice(['MM01', 'MM02'], n_rows),
            'NUMUNE_MIKTARI': np.random.randint(5, 50, n_rows),
            'MUAYENE_TIPI': np.random.choice(['Gorsel', 'Fonksiyonel', 'Tahribatli'], n_rows),
            'OPERASYON_NO': np.random.randint(10, 50, n_rows),
            'FLOW_END_DATE': pd.to_datetime(pd.Timestamp.now()) + pd.to_timedelta(np.random.rand(n_rows) * 10, unit='D'),
            'YARATILMA_TARIHI': pd.to_datetime(pd.Timestamp.now()) + pd.to_timedelta(np.random.rand(n_rows) * 5, unit='D'),
        }
        df = pd.DataFrame(data)
        df.loc[df.index < 10, 'NUMERIK_SONUC'] = 105.0
        return df

    # COMPREHENSIVE EDA
    def perform_eda(self):
        print("EXPLORATORY DATA ANALYSIS")
        df = self.df_raw
        print(f"\n Dataset Shape: {df.shape[0]} rows × {df.shape[1]} columns")
        missing = df.isnull().sum()
        missing_pct = (missing / len(df) * 100).round(2)
        missing_df = pd.DataFrame({
            'Missing_Count': missing,
            'Missing_Percentage': missing_pct
        }).sort_values('Missing_Count', ascending=False)
        print(f"\n Missing Values Summary:")
        print(missing_df[missing_df['Missing_Count'] > 0].head(10))
        self.eda_summary['total_records'] = len(df)
        self.eda_summary['total_features'] = len(df.columns)
        self.eda_summary['missing_values_total'] = missing.sum()
        if 'ACTUAL_DECISION' in df.columns:
            df['_TARGET_DECISION_NORM'] = df['ACTUAL_DECISION'].apply(self._normalize_label)
            decision_dist = df['_TARGET_DECISION_NORM'].value_counts()
            labeled_dist = decision_dist.drop(labels=['<<UNLABELED>>'], errors='ignore')
            print(f"\n Actual Decision (NIHAI_KARAR) Distribution (Normalized):")
            print(labeled_dist)
            labeled_count = df['ACTUAL_DECISION'].notnull().sum()
            unlabeled_count = df['ACTUAL_DECISION'].isnull().sum()
            print(f"\nLabeled records: {labeled_count}")
            print(f"Unlabeled records: {unlabeled_count}")
            self.eda_summary['labeled_records'] = labeled_count
            self.eda_summary['unlabeled_records'] = unlabeled_count
            self.eda_summary['decision_types'] = labeled_dist.to_dict()
            df.drop(columns=['_TARGET_DECISION_NORM'], inplace=True)
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if num_cols:
            print(f"\n Numerical Features Statistics:")
            print(df[num_cols].describe().round(2))
            for col in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT']:
                if col in df.columns:
                    self.eda_summary[f'{col}_mean'] = df[col].mean()
                    self.eda_summary[f'{col}_std'] = df[col].std()
        cat_cols = ['PROCESS_TYPE', 'DEFECT_TYPE', 'RESPONSIBLE_UNIT', 'STOCK_PLACE', 'MUAYENE_TIPI']
        for col in cat_cols:
            if col in df.columns:
                print(f"\n {col} Distribution:")
                dist = df[col].value_counts().head(10)
                print(dist)
                self.eda_summary[f'{col}_unique_count'] = df[col].nunique()
        if all(c in df.columns for c in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT']):
            out_of_limits = ((df['INSPECTION_RESULT'] < df['LOWER_LIMIT']) |
                            (df['INSPECTION_RESULT'] > df['UPPER_LIMIT'])).sum()
            ool_rate = out_of_limits / len(df) * 100
            print(f"\n  Out of Limits Analysis:")
            print(f"   Count: {out_of_limits}")
            print(f"   Rate: {ool_rate:.2f}%")
            self.eda_summary['out_of_limits_count'] = out_of_limits
            self.eda_summary['out_of_limits_rate'] = ool_rate
        if 'PROCESS_TIME' in df.columns and df['PROCESS_TIME'].sum() > 0:
            avg_time = df['PROCESS_TIME'].mean() / 3600
            print(f"\nAverage Process Time: {avg_time:.2f} hours")
            self.eda_summary['avg_process_time_hours'] = avg_time
        key_num_cols = [c for c in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT']
                        if c in df.columns]
        if len(key_num_cols) >= 2:
            corr = df[key_num_cols].corr()
            print(f"\n Key Features Correlation Matrix:")
            print(corr.round(3))
            self.eda_summary['correlation_matrix'] = corr.to_dict()
        logger.info("EDA completed successfully")
        return self

    # Feature Engineering Metotları
    def add_cost_center_features(self):
        df = self.df_raw
        weights = self.config.get('inspection_type_weights', {
            'Gorsel': 1.0, 'Fonksiyonel': 1.5, 'Tahribatli': 2.0
        })
        df['NUMUNE_MIKTARI'] = pd.to_numeric(df.get('NUMUNE_MIKTARI', 0), errors='coerce').fillna(0)
        df['MUAYENE_TIPI'] = df.get('MUAYENE_TIPI', 'Unknown')
        df['MM'] = df.get('MM', 'UNKNOWN')
        df['MUAYENE_TIPI_WEIGHT'] = df['MUAYENE_TIPI'].map(weights).fillna(1.0)
        df['COST_CENTER_LOAD'] = df['NUMUNE_MIKTARI'] * df['MUAYENE_TIPI_WEIGHT']
        total = df['COST_CENTER_LOAD'].sum()
        df['COST_CENTER_LOAD_NORM'] = df['COST_CENTER_LOAD'] / total if total > 0 else 0.0
        logger.info("Added cost center features")
        return self

    def add_transition_features(self):
        df = self.df_raw
        if 'STOCK_PLACE' not in df.columns and 'IS_EMRI_TAMAMLANMA_STOK_YERI' in df.columns:
            df['STOCK_PLACE'] = df['IS_EMRI_TAMAMLANMA_STOK_YERI']
        df['OPERASYON_NO'] = pd.to_numeric(df.get('OPERASYON_NO', 0), errors='coerce').fillna(0)
        df['STOCK_PLACE'] = df.get('STOCK_PLACE', 'UNKNOWN')
        df['STOK_YERI_ENC'] = df['STOCK_PLACE'].astype(str).astype('category').cat.codes
        df['TRANSITION_COST'] = abs(df['STOK_YERI_ENC'] - df['OPERASYON_NO'])
        df['BATCH_FLOW_SCORE'] = 1 / (1 + df['TRANSITION_COST'])
        mx = df['TRANSITION_COST'].max()
        df['TRANSITION_COST_NORM'] = df['TRANSITION_COST'] / mx if mx > 0 else 0.0
        logger.info("Added transition/batch flow features")
        return self

    def add_inefficiency_features(self):
        df = self.df_raw
        if all(c in df.columns for c in ['RESPONSIBLE_UNIT', 'PROCESS_TIME']):
            df['RESPONSIBLE_UNIT'] = df['RESPONSIBLE_UNIT'].astype(str)
            mean_time_per_unit = df.groupby('RESPONSIBLE_UNIT')['PROCESS_TIME'].mean()
            df['RESPONSIBLE_UNIT_INEFFICIENCY'] = df['RESPONSIBLE_UNIT'].map(mean_time_per_unit).fillna(0)
            min_val = df['RESPONSIBLE_UNIT_INEFFICIENCY'].min()
            max_val = df['RESPONSIBLE_UNIT_INEFFICIENCY'].max()
            if max_val > min_val:
                df['RESPONSIBLE_UNIT_INEFFICIENCY_NORM'] = (df['RESPONSIBLE_UNIT_INEFFICIENCY'] - min_val) / (max_val - min_val)
            else:
                df['RESPONSIBLE_UNIT_INEFFICIENCY_NORM'] = 0.0
            logger.info("Added responsible unit inefficiency features.")
        else:
            df['RESPONSIBLE_UNIT_INEFFICIENCY_NORM'] = 0.0
            logger.warning("Could not calculate inefficiency features. Missing RESPONSIBLE_UNIT or PROCESS_TIME.")
        return self

    def add_pca_features(self, n_components=1):
        if self.df_raw is None: raise ValueError("df_raw missing")
        num_cols = [c for c in ['INSPECTION_RESULT', 'UPPER_LIMIT', 'LOWER_LIMIT'] if c in self.df_raw.columns]
        if num_cols:
            clean = self.df_raw[num_cols].fillna(self.df_raw[num_cols].median())
            try:
                pca = PCA(n_components=min(n_components, clean.shape[1]))
                comp = pca.fit_transform(clean)
                self.df_raw['PCA_FEATURE'] = comp[:, 0]
                self.eda_summary['pca_explained_variance'] = float(pca.explained_variance_ratio_[0])
                logger.info(f"PCA explained variance: {pca.explained_variance_ratio_[0]:.4f}")
            except Exception:
                self.df_raw['PCA_FEATURE'] = 0.0
        else:
            self.df_raw['PCA_FEATURE'] = 0.0
        return self

    def add_mutual_info(self):
        if self.df_raw is None: raise ValueError("df_raw missing")
        df = self.df_raw
        if 'ACTUAL_DECISION' in df.columns:
            df['_TARGET_DECISION_NORM'] = df['ACTUAL_DECISION'].apply(self._normalize_label)
            labeled_mask = df['_TARGET_DECISION_NORM'] != '<<UNLABELED>>'
            if labeled_mask.sum() >= 5:
                candidate_cols = [c for c in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT', 'LIMIT_INTERVAL', 'DISTANCE_FROM_RESULT_MIDDLE_POINT', 'PROCESS_TIME'] if c in df.columns]
                if candidate_cols:
                    Xc = df.loc[labeled_mask, candidate_cols].fillna(0)
                    try:
                        le = LabelEncoder()
                        y_enc = le.fit_transform(df.loc[labeled_mask, '_TARGET_DECISION_NORM'])
                        mi = mutual_info_classif(Xc, y_enc, random_state=self.random_state)
                        mi_series = pd.Series(mi, index=candidate_cols)
                        total_mi = mi_series.sum()
                        if total_mi <= 0:
                            norm_mi = {c: 0.0 for c in candidate_cols}
                        else:
                            norm_mi = (mi_series / total_mi).to_dict()
                        mi_feat = Xc.apply(lambda r: sum(r[col] * norm_mi.get(col, 0) for col in candidate_cols), axis=1)
                        self.df_raw['MI_FEATURE'] = 0.0
                        self.df_raw.loc[labeled_mask, 'MI_FEATURE'] = mi_feat.values
                        self.eda_summary['mutual_info_scores'] = mi_series.to_dict()
                    except Exception:
                        self.df_raw['MI_FEATURE'] = 0.0
                else:
                    self.df_raw['MI_FEATURE'] = 0.0
            else:
                self.df_raw['MI_FEATURE'] = 0.0
            df.drop(columns=['_TARGET_DECISION_NORM'], inplace=True)
        else:
            self.df_raw['MI_FEATURE'] = 0.0
        return self

    def add_clusters(self, n_clusters=3):
        if self.df_raw is None: raise ValueError("df_raw missing")
        candidate_cols = [c for c in ['INSPECTION_RESULT', 'UPPER_LIMIT', 'LOWER_LIMIT', 'PCA_FEATURE'] if c in self.df_raw.columns]
        if candidate_cols and self.df_raw.shape[0] >= 1:
            Xc = self.df_raw[candidate_cols].fillna(0)
            try:
                nn = max(1, min(n_clusters, Xc.shape[0]))
                km = KMeans(n_clusters=nn, random_state=self.random_state, n_init='auto')
                self.df_raw['CLUSTER_ID'] = km.fit_predict(Xc)
                centers = km.cluster_centers_
                distances = []
                for i, row in Xc.iterrows():
                    cid = int(self.df_raw.at[i, 'CLUSTER_ID'])
                    distances.append(np.linalg.norm(row.values - centers[cid]))
                self.df_raw['CLUSTER_DISTANCE'] = distances
                cluster_dist = self.df_raw['CLUSTER_ID'].value_counts().to_dict()
                self.eda_summary['cluster_distribution'] = cluster_dist
            except Exception:
                self.df_raw['CLUSTER_ID'] = 0
                self.df_raw['CLUSTER_DISTANCE'] = 0.0
        else:
            self.df_raw['CLUSTER_ID'] = 0
            self.df_raw['CLUSTER_DISTANCE'] = 0.0
        return self

    def add_anomaly_score(self, contamination=0.03):
        if self.df_raw is None: raise ValueError("df_raw missing")
        candidate_cols = [c for c in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT', 'DISTANCE_FROM_RESULT_MIDDLE_POINT', 'PROCESS_TIME'] if c in self.df_raw.columns]
        if candidate_cols and self.df_raw.shape[0] >= 5:
            Xc = self.df_raw[candidate_cols].fillna(0)
            try:
                iso = IsolationForest(contamination=contamination, random_state=self.random_state)
                pred = iso.fit_predict(Xc)
                self.df_raw['ANOMALY_FLAG'] = (pred == -1).astype(int)
                try:
                    self.df_raw['ANOMALY_SCORE'] = iso.decision_function(Xc)
                except Exception:
                    self.df_raw['ANOMALY_SCORE'] = self.df_raw['ANOMALY_FLAG']
                anomaly_rate = self.df_raw['ANOMALY_FLAG'].mean() * 100
                self.eda_summary['anomaly_detection_rate'] = anomaly_rate
                logger.info(f"Anomaly detection rate: {anomaly_rate:.2f}%")
            except Exception:
                self.df_raw['ANOMALY_FLAG'] = 0
                self.df_raw['ANOMALY_SCORE'] = 0.0
        else:
            self.df_raw['ANOMALY_FLAG'] = 0
            self.df_raw['ANOMALY_SCORE'] = 0.0
        return self

    def add_corr_feature(self):
        if self.df_raw is None: raise ValueError("df_raw missing")
        candidate_cols = [c for c in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT'] if c in self.df_raw.columns]
        if len(candidate_cols) >= 2:
            try:
                corr_mat = self.df_raw[candidate_cols].corr().abs()
                self.df_raw['CORR_LEVEL'] = corr_mat.mean().mean()
            except Exception:
                self.df_raw['CORR_LEVEL'] = 0.0
        else:
            self.df_raw['CORR_LEVEL'] = 0.0
        return self

    # Prepare for modeling
    def prepare_for_modeling(self, test_size=0.2):
        if self.df_raw is None: raise ValueError("df_raw missing")
        df = self.df_raw.copy()
        if 'INSPECTION_RESULT' in df.columns:
            df['INSPECTION_RESULT'] = pd.to_numeric(df['INSPECTION_RESULT'], errors='coerce')
        if {'UPPER_LIMIT', 'LOWER_LIMIT'}.issubset(df.columns):
            df['LIMIT_INTERVAL'] = df['UPPER_LIMIT'] - df['LOWER_LIMIT']
            df['MIDDLE_POINT'] = (df['UPPER_LIMIT'] + df['LOWER_LIMIT']) / 2
            if 'INSPECTION_RESULT' in df.columns:
                df['DISTANCE_FROM_RESULT_MIDDLE_POINT'] = (df['INSPECTION_RESULT'] - df['MIDDLE_POINT']).abs()
            else:
                df['DISTANCE_FROM_RESULT_MIDDLE_POINT'] = 0.0
            df['OUT_OF_LIMITS'] = ((df['INSPECTION_RESULT'] < df['LOWER_LIMIT']) | (df['INSPECTION_RESULT'] > df['UPPER_LIMIT'])).astype(int)
        else:
            df['LIMIT_INTERVAL'] = 0.0
            df['DISTANCE_FROM_RESULT_MIDDLE_POINT'] = 0.0
            df['OUT_OF_LIMITS'] = 0
        feature_cols = []
        for name in ['PROCESS_TYPE', 'INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT', 'DEFECT_TYPE',
                     'RESPONSIBLE_UNIT', 'STOCK_PLACE', 'LIMIT_INTERVAL', 'DISTANCE_FROM_RESULT_MIDDLE_POINT',
                     'OUT_OF_LIMITS', 'PROCESS_TIME', 'PCA_FEATURE', 'MI_FEATURE', 'CLUSTER_ID',
                     'CLUSTER_DISTANCE', 'ANOMALY_FLAG', 'ANOMALY_SCORE', 'CORR_LEVEL', 'DECISION_TYPE']:
            if name in df.columns:
                feature_cols.append(name)
        for name in ['NUMUNE_MIKTARI', 'MUAYENE_TIPI_WEIGHT', 'COST_CENTER_LOAD', 'COST_CENTER_LOAD_NORM',
                     'OPERASYON_NO', 'STOK_YERI_ENC', 'TRANSITION_COST', 'BATCH_FLOW_SCORE', 'TRANSITION_COST_NORM',
                     'RESPONSIBLE_UNIT_INEFFICIENCY_NORM']:
            if name in df.columns and name not in feature_cols:
                feature_cols.append(name)
        self.feature_cols = feature_cols
        categorical_cols = [c for c in ['PROCESS_TYPE', 'DEFECT_TYPE', 'RESPONSIBLE_UNIT', 'STOCK_PLACE', 'MUAYENE_TIPI', 'DECISION_TYPE'] if c in df.columns]
        for col in categorical_cols:
            le = LabelEncoder()
            try:
                df[col] = le.fit_transform(df[col].astype(str))
                self.label_encoders[col] = le
            except Exception:
                df[col] = 0
        if 'ACTUAL_DECISION' in df.columns:
            df['ACTUAL_DECISION_NORM'] = df['ACTUAL_DECISION'].apply(self._normalize_label)
            df['ACTUAL_DECISION_ENC'] = -1
            valid_mask = df['ACTUAL_DECISION_NORM'] != '<<UNLABELED>>'
            if valid_mask.sum() > 0:
                le = LabelEncoder()
                unique_norm = sorted(df.loc[valid_mask, 'ACTUAL_DECISION_NORM'].unique(),
                                     key=lambda s: int(s.split('-')[-1]) if s.startswith('Karar-') and s.split('-')[-1].isdigit() else s)
                le.fit(unique_norm)
                df.loc[valid_mask, 'ACTUAL_DECISION_ENC'] = le.transform(df.loc[valid_mask, 'ACTUAL_DECISION_NORM'])
                self.label_encoders['TARGET_DECISION'] = le
        else:
            df['ACTUAL_DECISION_ENC'] = -1
        self.df_model = df
        labeled = df[df['ACTUAL_DECISION_ENC'] != -1].copy()
        if labeled.empty:
            self.X_train = pd.DataFrame(columns=feature_cols)
            self.X_test = pd.DataFrame(columns=feature_cols)
            self.y_train = pd.Series(dtype=int)
            self.y_test = pd.Series(dtype=int)
            logger.info("No labeled rows for training.")
            return self
        X = labeled[feature_cols].fillna(0)
        y = labeled['ACTUAL_DECISION_ENC'].astype(int)
        stratify_arg = y if (len(y.value_counts()) > 1 and (y.value_counts() > 1).all()) else None
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=self.random_state, stratify=stratify_arg
        )
        self.scaler = StandardScaler()
        num_cols = [c for c in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT', 'LIMIT_INTERVAL',
                                'DISTANCE_FROM_RESULT_MIDDLE_POINT', 'PROCESS_TIME', 'PCA_FEATURE',
                                'MI_FEATURE', 'ANOMALY_SCORE', 'CORR_LEVEL', 'CLUSTER_DISTANCE',
                                'NUMUNE_MIKTARI', 'MUAYENE_TIPI_WEIGHT', 'COST_CENTER_LOAD',
                                'OPERASYON_NO', 'TRANSITION_COST', 'BATCH_FLOW_SCORE',
                                'RESPONSIBLE_UNIT_INEFFICIENCY_NORM'] if c in feature_cols]
        if num_cols and not self.X_train.empty:
            try:
                self.X_train.loc[:, num_cols] = self.scaler.fit_transform(self.X_train[num_cols])
                self.X_test.loc[:, num_cols] = self.scaler.transform(self.X_test[num_cols])
            except Exception:
                logger.warning("Scaling failed; continuing without scaling numeric columns.")
        logger.info("Prepared modeling dataset; features: %d", len(feature_cols))
        return self

    # Train models
    def train_rf_model(self, n_estimators=200, max_depth=15):
        if self.X_train is None or self.X_train.empty:
            logger.warning("Train set empty — RF not trained.")
            return self
        self.rf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                              min_samples_split=5, min_samples_leaf=2,
                                              random_state=self.random_state, n_jobs=-1,
                                              class_weight='balanced')
        self.rf_model.fit(self.X_train, self.y_train)
        try:
            cv_scores = cross_val_score(self.rf_model, self.X_train, self.y_train, cv=5)
            logger.info("RF CV Acc: %.4f ± %.4f", cv_scores.mean(), cv_scores.std())
            self.model_metrics['rf_cv_mean'] = cv_scores.mean()
            self.model_metrics['rf_cv_std'] = cv_scores.std()
        except Exception:
            pass
        return self

    def train_xgboost_model(self):
        if XGBClassifier is None:
            logger.info("XGBoost not installed; skipping XGBoost training.")
            self.xgb_model = None
            return self
        if self.X_train is None or self.X_train.empty:
            logger.warning("Train set empty — XGB not trained.")
            return self
        try:
            num_classes = len(np.unique(self.y_train)) if self.y_train is not None and len(self.y_train) > 0 else 2
            self.xgb_model = XGBClassifier(objective='multi:softprob', num_class=max(2, num_classes),
                                          n_estimators=250, learning_rate=0.1, max_depth=6,
                                          subsample=0.8, colsample_bytree=0.8,
                                          random_state=self.random_state, use_label_encoder=False,
                                          eval_metric='mlogloss')
            self.xgb_model.fit(self.X_train, self.y_train)
        except Exception:
            logger.warning("XGBoost training failed; skipping.")
            self.xgb_model = None
        return self

    def train_logistic_model(self):
        if self.X_train is None or self.X_train.empty:
            logger.warning("Train set empty — LR not trained.")
            return self
        try:
            self.logreg_model = LogisticRegression(multi_class='multinomial', solver='lbfgs',
                                                  max_iter=500, class_weight='balanced')
            self.logreg_model.fit(self.X_train, self.y_train)
        except Exception:
            logger.warning("LogisticRegression training failed; skipping.")
            self.logreg_model = None
        return self

    # MODEL PERFORMANCE METRICS
    def calculate_model_performance(self):
        print("MODEL PERFORMANCE EVALUATION (TEST SET)")
        print("  NOTE: These metrics are on the test set split (~20% of labeled data)")
        print("   Full dataset metrics will be shown in QA Metrics section")
        if self.X_test is None or self.X_test.empty:
            logger.warning("No test set available for performance evaluation")
            return self
        self.model_metrics['test_set_size'] = len(self.X_test)
        self.model_metrics['train_set_size'] = len(self.X_train) if self.X_train is not None else 0
        try:
            class_names = list(self.label_encoders['TARGET_DECISION'].classes_)
        except Exception:
            class_names = [f'Class_{i}' for i in range(len(self.y_test.unique()))]
        target_names = class_names
        def evaluate_model(name, model, prefix):
            if model is None:
                print(f"\n{name}: Not available")
                return
            try:
                y_pred = model.predict(self.X_test)
                acc = accuracy_score(self.y_test, y_pred)
                f1_macro = f1_score(self.y_test, y_pred, average='macro', zero_division=0)
                f1_weighted = f1_score(self.y_test, y_pred, average='weighted', zero_division=0)
                precision, recall, f1, support = precision_recall_fscore_support(
                    self.y_test, y_pred, average=None, zero_division=0
                )
                self.model_metrics[f'{prefix}_accuracy'] = float(acc)
                self.model_metrics[f'{prefix}_f1_macro'] = float(f1_macro)
                self.model_metrics[f'{prefix}_f1_weighted'] = float(f1_weighted)
                self.model_metrics[f'{prefix}_precision_macro'] = float(precision.mean())
                self.model_metrics[f'{prefix}_recall_macro'] = float(recall.mean())
                cm = confusion_matrix(self.y_test, y_pred)
                self.confusion_matrices[prefix] = cm
                print(f"\n{name} Performance")
                print(classification_report(self.y_test, y_pred, target_names=target_names, zero_division=0))
                unique_classes = sorted(np.unique(self.y_test))
                for i, class_code in enumerate(unique_classes):
                    try:
                        class_name = target_names[class_code]
                        self.model_metrics[f'{prefix}_{class_name}_precision'] = float(precision[i])
                        self.model_metrics[f'{prefix}_{class_name}_recall'] = float(recall[i])
                        self.model_metrics[f'{prefix}_{class_name}_f1'] = float(f1[i])
                    except:
                         pass
                print(f"\nConfusion Matrix:")
                print(cm)
            except Exception as e:
                logger.error(f"Error evaluating {name}: {e}")
        evaluate_model("Random Forest", self.rf_model, "rf")
        evaluate_model("XGBoost", self.xgb_model, "xgb")
        evaluate_model("Logistic Regression", self.logreg_model, "lr")
        logger.info("Model performance evaluation completed")
        return self

    # FEATURE IMPORTANCE ANALYSIS
    def analyze_feature_importance(self):
        print("FEATURE IMPORTANCE ANALYSIS")
        importance_data = []
        if self.rf_model and hasattr(self.rf_model, 'feature_importances_'):
            rf_importance = pd.DataFrame({
                'feature': self.feature_cols,
                'rf_importance': self.rf_model.feature_importances_
            })
            importance_data.append(rf_importance)
            print("\nRandom Forest - Top 15 Most Important Features:")
            top_rf = rf_importance.sort_values('rf_importance', ascending=False).head(15)
            for idx, row in top_rf.iterrows():
                print(f"  {row['feature']:<40} {row['rf_importance']:.6f}")
        if self.xgb_model and hasattr(self.xgb_model, 'feature_importances_'):
            xgb_importance = pd.DataFrame({
                'feature': self.feature_cols,
                'xgb_importance': self.xgb_model.feature_importances_
            })
            importance_data.append(xgb_importance)
            print("\nXGBoost - Top 15 Most Important Features:")
            top_xgb = xgb_importance.sort_values('xgb_importance', ascending=False).head(15)
            for idx, row in top_xgb.iterrows():
                print(f"  {row['feature']:<40} {row['xgb_importance']:.6f}")
        if importance_data:
            self.feature_importance = importance_data[0]
            for df in importance_data[1:]:
                self.feature_importance = self.feature_importance.merge(df, on='feature', how='outer')
            imp_cols = [c for c in self.feature_importance.columns if 'importance' in c]
            if imp_cols:
                self.feature_importance['avg_importance'] = self.feature_importance[imp_cols].mean(axis=1)
                self.feature_importance = self.feature_importance.sort_values('avg_importance', ascending=False)
                print("\nCombined Feature Importance (Average):")
                for idx, row in self.feature_importance.head(15).iterrows():
                    print(f"  {row['feature']:<40} {row['avg_importance']:.6f}")
                top_features = self.feature_importance.head(10)['feature'].tolist()
                self.eda_summary['top_10_features'] = top_features
        logger.info("Feature importance analysis completed")
        return self

    # Optimization Decision Helpers
    def optimize_decision(self, prediction_probs: np.ndarray, class_names: List[str], weights: Optional[Dict[int, float]] = None) -> Optional[str]:
        if weights is None:
            weights = {i: 1.0 for i in range(len(class_names))}
        n = len(class_names)
        x = pulp.LpVariable.dicts("x", list(range(n)), cat='Binary')
        prob = pulp.LpProblem("OR", pulp.LpMaximize)
        prob += pulp.lpSum([float(prediction_probs[i]) * weights.get(i, 1.0) * x[i] for i in range(n)])
        prob += pulp.lpSum([x[i] for i in range(n)]) == 1
        prob.solve(pulp.PULP_CBC_CMD(msg=False))
        for i in range(n):
            if pulp.value(x[i]) == 1:
                return class_names[i]
        return None

    def deterministic_decision(self, probs: np.ndarray, class_names: List[str], cost: np.ndarray, quality: np.ndarray, capacity: np.ndarray) -> Optional[str]:
        n = len(class_names)
        cost = cost[:n]; quality = quality[:n]; capacity = capacity[:n]
        x = pulp.LpVariable.dicts("x", range(n), cat='Binary')
        prob = pulp.LpProblem("Deterministic", pulp.LpMaximize)
        prob += pulp.lpSum([float(probs[i]) * (quality[i] - cost[i]) * x[i] for i in range(n)])
        prob += pulp.lpSum([x[i] for i in range(n)]) == 1
        prob += pulp.lpSum([capacity[i] * x[i] for i in range(n)]) <= 1
        prob.solve(pulp.PULP_CBC_CMD(msg=False))
        for i in range(n):
            if pulp.value(x[i]) == 1:
                return class_names[i]
        return None

    def robust_decision(self, probs: np.ndarray, class_names: List[str], penalty=0.15) -> Optional[str]:
        n = len(class_names)
        p = np.array(probs, dtype=float)
        scenarios = [p, np.clip(p - penalty, 0, 1), np.clip(p + penalty, 0, 1)]
        x = pulp.LpVariable.dicts("x", range(n), cat='Binary')
        prob = pulp.LpProblem("Robust", pulp.LpMaximize)
        prob += pulp.lpSum([min(s[i] for s in scenarios) * x[i] for i in range(n)])
        prob += pulp.lpSum([x[i] for i in range(n)]) == 1
        prob.solve(pulp.PULP_CBC_CMD(msg=False))
        for i in range(n):
            if pulp.value(x[i]) == 1:
                return class_names[i]
        return None

    def _get_decision_cost(self, decision_name: str) -> float:
        handling_costs = self.config.get('handling_costs', {
            'KABUL': 0.0, 'RTV': 1.0, 'RU': 2.0, 'ISLAH': 5.0, 'HURDA': 10.0
        })
        cost_vector = self.config.get('cost_vector', [])
        cls_upper = str(decision_name).upper()
        for key, cost in handling_costs.items():
            if key in cls_upper:
                return cost
        try:
            if 'KARAR-' in cls_upper:
                le = self.label_encoders.get('TARGET_DECISION')
                if le:
                    norm_label = self._normalize_label(decision_name)
                    if norm_label in le.classes_:
                        encoded_val = le.transform([norm_label])[0]
                        return cost_vector[encoded_val] if 0 <= encoded_val < len(cost_vector) else 5.0
        except Exception:
            pass
        return 5.0

    # Optimization Decisions (Score Producers)
    def optimize_decision_model1(self, prediction_probs: np.ndarray, class_names: List[str], row: pd.Series) -> float:
        n = len(class_names)
        F_i = self.config.get('defect_financial_weight', 10.0)
        handling_costs = self.config.get('handling_costs', {
            'KABUL': 0.0, 'RTV': 1.0, 'RU': 2.0, 'ISLAH': 5.0, 'HURDA': 10.0
        })
        cost_vector = self.config.get('cost_vector', [])
        risk_costs = []
        for i, cls in enumerate(class_names):
            cls_upper = str(cls).upper()
            cost = 5.0
            if 'KABUL' in cls_upper or 'ACCEPT' in cls_upper: cost = handling_costs.get('KABUL', 0.0)
            elif 'RTV' in cls_upper: cost = handling_costs.get('RTV', 1.0)
            elif 'RU' in cls_upper: cost = handling_costs.get('RU', 2.0)
            elif 'ISLAH' in cls_upper or 'REWORK' in cls_upper: cost = handling_costs.get('ISLAH', 5.0)
            elif 'HURDA' in cls_upper or 'SCRAP' in cls_upper: cost = handling_costs.get('HURDA', 10.0)
            if cost == 5.0 or 'KARAR-' in cls_upper:
                try:
                    le = self.label_encoders.get('TARGET_DECISION')
                    if le:
                        encoded_val = le.transform([self._normalize_label(cls)])[0]
                        cost = cost_vector[encoded_val] if 0 <= encoded_val < len(cost_vector) else cost
                except Exception:
                    pass
            risk_costs.append(cost * float(prediction_probs[i]) * F_i)
        return min(risk_costs) if risk_costs else 0.0

    def cost_center_balancing_decision_model2(self, probs: np.ndarray, class_names: List[str], row: pd.Series) -> float:
        mm = str(row.get('MM', 'UNKNOWN'))
        muayene_tipi = str(row.get('MUAYENE_TIPI', 'Gorsel'))
        sample_size = float(row.get('NUMUNE_MIKTARI', 1))
        capacities = self.config.get('cost_center_capacity', {mm: 1000})
        Cap_k = capacities.get(mm, 1000)
        workload_map = self.config.get('inspection_workload_hours', {
            'Gorsel': 0.5, 'Fonksiyonel': 1.5, 'Tahribatli': 3.0
        })
        w_j = workload_map.get(muayene_tipi, 1.0)
        load = sample_size * w_j
        load_ratio = load / Cap_k if Cap_k > 0 else 1.0
        return load_ratio

    def batch_flow_optimization_decision_model3(self, probs: np.ndarray, class_names: List[str], row: pd.Series) -> float:
        flow = float(row.get('BATCH_FLOW_SCORE', 0))
        return flow

    def anomaly_informed_assignment_decision_model4(self, probs: np.ndarray, class_names: List[str], row: pd.Series) -> float:
        anomaly_flag = float(row.get('ANOMALY_FLAG', 0))
        inefficiency_norm = float(row.get('RESPONSIBLE_UNIT_INEFFICIENCY_NORM', 0))
        risk_factor = anomaly_flag + inefficiency_norm
        return risk_factor

    # COMPREHENSIVE QA METRICS
    def add_comprehensive_qa_metrics(self):
        if self.results_df is None or 'ACTUAL_DECISION' not in self.results_df.columns:
            logger.info("No actual decisions available for QA metrics")
            return self
        df = self.results_df
        actual_mask = df['ACTUAL_DECISION'].notnull()
        if actual_mask.sum() == 0:
            logger.info("No labeled actual decisions for QA comparison")
            return self
        print("COMPREHENSIVE QA METRICS ANALYSIS (FULL DATASET)")
        has_encoder = 'TARGET_DECISION' in self.label_encoders
        if has_encoder:
            encoder = self.label_encoders['TARGET_DECISION']
            print(f"\n Target Decision Classes (Normalized): {list(encoder.classes_)}")
        else:
            print("\n  Warning: No label encoder found for TARGET_DECISION")
            logger.warning("Cannot properly decode predictions without target label encoder")
            return self
        def decode_prediction(value):
            if pd.isna(value) or value is None: return None
            if isinstance(value, str):
                normalized = self._normalize_label(value)
                if normalized == '<<UNLABELED>>': return None
                if normalized in encoder.classes_: return normalized
                try: value = int(value)
                except: return normalized
            if isinstance(value, (int, np.integer)):
                try:
                    if 0 <= value < len(encoder.classes_):
                        return encoder.classes_[value]
                except: pass
            return str(value)
        decision_methods = ['RF_PRED', 'XGB_PRED', 'LR_PRED', 'OR_DECISION',
                           'DETERMINISTIC_DECISION', 'ROBUST_DECISION',
                           'HYBRID_OPTIMIZATION_DECISION']
        print("\n Decoding predictions to original normalized labels...")
        for method in decision_methods:
            if method in df.columns:
                df[f'{method}_DECODED'] = df[method].apply(decode_prediction)
        df['ACTUAL_DECISION_NORM'] = df['ACTUAL_DECISION'].apply(self._normalize_label)
        actual_mask_norm = df['ACTUAL_DECISION_NORM'] != '<<UNLABELED>>'
        qa_summary_data = []
        for method in decision_methods:
            decoded_col = f'{method}_DECODED'
            if decoded_col not in df.columns: continue
            valid_mask = actual_mask_norm & df[decoded_col].notnull()
            if valid_mask.sum() < 2:
                print(f"\n  {method}: Not enough valid samples ({valid_mask.sum()})")
                continue
            actual = df.loc[valid_mask, 'ACTUAL_DECISION_NORM'].astype(str)
            predicted = df.loc[valid_mask, decoded_col].astype(str)
            try:
                report = classification_report(actual, predicted, zero_division=0, output_dict=True)
                acc = report['accuracy']
                precision_macro = report['macro avg']['precision']
                recall_macro = report['macro avg']['recall']
                f1_macro = report['macro avg']['f1-score']
                precision_weighted = report['weighted avg']['precision']
                recall_weighted = report['weighted avg']['recall']
                f1_weighted = report['weighted avg']['f1-score']
                qa_summary_data.append({
                    'Decision_Method': method, 'Accuracy': acc, 'Precision_Macro': precision_macro,
                    'Recall_Macro': recall_macro, 'F1_Macro': f1_macro, 'Precision_Weighted': precision_weighted,
                    'Recall_Weighted': recall_weighted, 'F1_Weighted': f1_weighted, 'Sample_Size': valid_mask.sum()
                })
                print(f"\n{method} Metrics:")
                print(f"  Sample Size:        {valid_mask.sum()}")
                print(classification_report(actual, predicted, zero_division=0))
                for label, metrics in report.items():
                    if label not in ['accuracy', 'macro avg', 'weighted avg']:
                        self.model_metrics[f'{method}_{label}_precision'] = float(metrics['precision'])
                        self.model_metrics[f'{method}_{label}_recall'] = float(metrics['recall'])
                        self.model_metrics[f'{method}_{label}_f1'] = float(metrics['f1-score'])
                unique_labels = sorted(set(actual.unique()) | set(predicted.unique()))
                cm = confusion_matrix(actual, predicted, labels=unique_labels)
                cm_df = pd.DataFrame(cm, index=unique_labels, columns=unique_labels)
                print(f"\n  Confusion Matrix (Rows=Actual, Cols=Predicted):")
                print(f"  {cm_df.to_string()}")
            except Exception as e:
                logger.error(f"Error calculating metrics for {method}: {e}")
                import traceback
                traceback.print_exc()
        if qa_summary_data:
            self.qa_metrics_summary = pd.DataFrame(qa_summary_data)
            print("\nQA METHODS COMPARISON SUMMARY")
            print(self.qa_metrics_summary.to_string(index=False))
            best_method = self.qa_metrics_summary.loc[self.qa_metrics_summary['F1_Macro'].idxmax()]
            print(f"\n Best Performing Method: {best_method['Decision_Method']}")
            print(f"   Accuracy: {best_method['Accuracy']:.4f}")
            print(f"   F1-Macro: {best_method['F1_Macro']:.4f}")
        else:
            print("\n  No QA metrics could be calculated. Check data and predictions.")
        df.drop(columns=['ACTUAL_DECISION_NORM'], inplace=True)
        for method in decision_methods:
            if f'{method}_DECODED' in df.columns:
                 df.drop(columns=[f'{method}_DECODED'], inplace=True)
        logger.info("Comprehensive QA metrics completed")
        return self

    # Process all data -> predictions, ensemble, QA
    def process_all_data(self):
        if self.df_model is None:
            raise ValueError("df_model missing. Call prepare_for_modeling() first.")
        df = self.df_model.copy()
        feature_cols = self.feature_cols or [c for c in ['PROCESS_TYPE', 'INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT', 'DEFECT_TYPE', 'RESPONSIBLE_UNIT', 'STOCK_PLACE', 'LIMIT_INTERVAL', 'DISTANCE_FROM_RESULT_MIDDLE_POINT', 'OUT_OF_LIMITS', 'PROCESS_TIME', 'PCA_FEATURE', 'MI_FEATURE', 'CLUSTER_ID', 'CLUSTER_DISTANCE', 'ANOMALY_FLAG', 'ANOMALY_SCORE', 'CORR_LEVEL'] if c in df.columns]

        if self.scaler is not None and hasattr(self.scaler, 'mean_'):
            num_cols = [c for c in ['INSPECTION_RESULT', 'LOWER_LIMIT', 'UPPER_LIMIT', 'LIMIT_INTERVAL', 'DISTANCE_FROM_RESULT_MIDDLE_POINT', 'PROCESS_TIME', 'PCA_FEATURE', 'MI_FEATURE', 'ANOMALY_SCORE', 'CORR_LEVEL', 'CLUSTER_DISTANCE', 'NUMUNE_MIKTARI', 'MUAYENE_TIPI_WEIGHT', 'COST_CENTER_LOAD', 'OPERASYON_NO', 'TRANSITION_COST', 'BATCH_FLOW_SCORE', 'RESPONSIBLE_UNIT_INEFFICIENCY_NORM'] if c in feature_cols]
            try:
                if num_cols:
                    df.loc[:, num_cols] = self.scaler.transform(df[num_cols].fillna(0))
            except Exception:
                logger.warning("Full-scaling failed; continuing without.")

        X_all = df[feature_cols].fillna(0)
        n_rows = len(X_all)

        class_names: List[str] = []
        try:
            if 'TARGET_DECISION' in self.label_encoders:
                class_names = list(self.label_encoders['TARGET_DECISION'].classes_)
        except Exception:
            pass
        if not class_names:
            num_classes = 2
            class_names = [f'Karar-{i+1}' for i in range(num_classes)]

        n_classes = len(class_names)

        def safe_proba(model, X):
            if model is None: return np.zeros((len(X), n_classes))
            try:
                p = model.predict_proba(X)
                if p.ndim == 1: p = p.reshape(-1, 1)
                if p.shape[1] != n_classes:
                    new = np.zeros((p.shape[0], n_classes)); minc = min(p.shape[1], n_classes)
                    new[:, :minc] = p[:, :minc]; p = new
                return p
            except Exception: return np.zeros((len(X), n_classes))

        def safe_pred(model, X):
            if model is None: return np.array([None] * len(X))
            try: return model.predict(X)
            except Exception: return np.array([None] * len(X))

        # DRY Prensibi: Modelleri bir döngüde çalıştırma
        models = {
            'rf': self.rf_model,
            'xgb': self.xgb_model,
            'lr': self.logreg_model
        }
        all_probs = {}
        all_preds = {}

        for name, model in models.items():
            all_probs[name] = safe_proba(model, X_all)
            all_preds[name] = safe_pred(model, X_all)

        probs_rf = all_probs['rf']
        probs_xgb = all_probs['xgb']
        probs_lr = all_probs['lr']
        preds_rf = all_preds['rf']
        preds_xgb = all_preds['xgb']
        preds_lr = all_preds['lr']

        results = []
        for idx in range(n_rows):
            row = df.iloc[idx]
            actual_label = row.get('ACTUAL_DECISION_NORM', row.get('ACTUAL_DECISION', None))
            actual = actual_label if pd.notnull(actual_label) and actual_label != '<<UNLABELED>>' else None

            def pred_name(pred):
                try:
                    if pred is None: return None
                    if isinstance(pred, (np.integer, int)):
                        p = int(pred)
                        if 'TARGET_DECISION' in self.label_encoders:
                            le = self.label_encoders['TARGET_DECISION']
                            if 0 <= p < len(le.classes_):
                                return le.classes_[p]
                        return str(pred)
                    if isinstance(pred, str): return pred
                    return str(pred)
                except Exception: return None

            # Tahminleri all_preds diktelerinden çekme
            rf_pred = pred_name(preds_rf[idx]) if len(preds_rf) > idx else None
            xgb_pred = pred_name(preds_xgb[idx]) if len(preds_xgb) > idx else None
            lr_pred = pred_name(preds_lr[idx]) if len(preds_lr) > idx else None

            prob_rf = probs_rf[idx] if probs_rf.shape[0] > 0 else np.zeros(n_classes)
            prob_xgb = probs_xgb[idx] if probs_xgb.shape[0] > 0 else prob_rf
            prob_lr = probs_lr[idx] if probs_lr.shape[0] > 0 else prob_rf

            def normalize(p):
                p = np.array(p, dtype=float)
                s = p.sum()
                if s <= 0: return np.ones_like(p) / max(1, len(p))
                return p / s

            prf = normalize(prob_rf)
            pxgb = normalize(prob_xgb)
            plr = normalize(prob_lr)
            ensemble = (prf + pxgb + plr) / 3.0

            try:
                model1_copq_risk_score = self.optimize_decision_model1(ensemble, class_names, row)
            except Exception: model1_copq_risk_score = 0.0

            try:
                model2_cc_overload_penalty = self.cost_center_balancing_decision_model2(ensemble, class_names, row)
            except Exception: model2_cc_overload_penalty = 0.0

            try:
                model3_batch_flow_factor = self.batch_flow_optimization_decision_model3(ensemble, class_names, row)
            except Exception: model3_batch_flow_factor = 0.0

            try:
                model4_assignment_risk_factor = self.anomaly_informed_assignment_decision_model4(ensemble, class_names, row)
            except Exception: model4_assignment_risk_factor = 0.0


            # Konfigürasyondan Ağırlıkları Çekme
            W_COPQ = self.config.get('W_COPQ', 1.5)
            W_CC = self.config.get('W_CC', 1.0)
            W_FLOW = self.config.get('W_FLOW', 1.5)
            W_ASSIGN = self.config.get('W_ASSIGN', 2.0)

            final_scores = {}
            best_score = -np.inf
            hybrid_decision = None

            for i, cls in enumerate(class_names):
                prob = ensemble[i]
                unit_cost = self._get_decision_cost(cls)
                copq_penalty = W_COPQ * unit_cost * prob
                overall_risk_penalty = (
                    W_CC * model2_cc_overload_penalty +
                    W_ASSIGN * model4_assignment_risk_factor
                )
                overall_reward = W_FLOW * model3_batch_flow_factor
                final_score = prob * (1 + overall_reward) - (copq_penalty + overall_risk_penalty)
                final_scores[cls] = final_score

                if final_score > best_score:
                    best_score = final_score
                    hybrid_decision = cls


            try: or_decision = self.optimize_decision(ensemble, class_names)
            except Exception: or_decision = None

            try:
                cost_vec = np.array(self.config.get('cost_vector', [0.0] * n_classes), dtype=float)
                quality_vec = np.array(self.config.get('quality_vector', [1.0] * n_classes), dtype=float)
                capacity_vec = np.array(self.config.get('capacity_vector', [1.0] * n_classes), dtype=float)
                cost_vec = cost_vec[:n_classes]
                quality_vec = quality_vec[:n_classes]
                capacity_vec = capacity_vec[:n_classes]
            except Exception:
                cost_vec = np.zeros(n_classes)
                quality_vec = np.ones(n_classes)
                capacity_vec = np.ones(n_classes)

            try: det_decision = self.deterministic_decision(ensemble, class_names, cost=cost_vec, quality=quality_vec, capacity=capacity_vec)
            except Exception: det_decision = None
            try: rob_decision = self.robust_decision(ensemble, class_names)
            except Exception: rob_decision = None


            result = {
                'WORK_ORDER': self.df_raw.iloc[idx].get('WORK_ORDER', None),
                'ACTUAL_DECISION': actual,
                'RF_PRED': rf_pred,
                'XGB_PRED': xgb_pred,
                'LR_PRED': lr_pred,
                'HYBRID_OPTIMIZATION_DECISION': hybrid_decision,
                'MODEL1_COPQ_RISK_SCORE': model1_copq_risk_score,
                'MODEL2_CC_OVERLOAD_PENALTY': model2_cc_overload_penalty,
                'MODEL3_BATCH_FLOW_FACTOR': model3_batch_flow_factor,
                'MODEL4_ASSIGNMENT_RISK_FACTOR': model4_assignment_risk_factor,
                'OR_DECISION': or_decision,
                'DETERMINISTIC_DECISION': det_decision,
                'ROBUST_DECISION': rob_decision,
            }

            for i, cls in enumerate(class_names):
                result[f'PROB_RF_{cls}'] = float(prf[i]) if i < len(prf) else 0.0
                result[f'PROB_XGB_{cls}'] = float(pxgb[i]) if i < len(pxgb) else result[f'PROB_RF_{cls}']
                result[f'PROB_LR_{cls}'] = float(plr[i]) if i < len(plr) else result[f'PROB_RF_{cls}']
                result[f'ENSEMBLE_PROB_{cls}'] = float(ensemble[i]) if i < len(ensemble) else 0.0

            result['OUT_OF_LIMITS'] = int(row.get('OUT_OF_LIMITS', 0) or 0)
            result['ANOMALY_FLAG'] = int(row.get('ANOMALY_FLAG', 0) or 0)
            result['ANOMALY_SCORE'] = float(row.get('ANOMALY_SCORE', 0) or 0.0)
            result['DISTANCE_FROM_RESULT_MIDDLE_POINT'] = float(row.get('DISTANCE_FROM_RESULT_MIDDLE_POINT', 0) or 0.0)
            result['CLUSTER_ID'] = row.get('CLUSTER_ID', None)
            result['CLUSTER_DISTANCE'] = float(row.get('CLUSTER_DISTANCE', 0) or 0.0)

            try: result['qa_prob_vector'] = json.dumps({class_names[i]: float(ensemble[i]) for i in range(len(class_names))})
            except Exception: result['qa_prob_vector'] = None
            try: result['qa_model_predictions'] = json.dumps({'rf': rf_pred, 'xgb': xgb_pred, 'lr': lr_pred})
            except Exception: result['qa_model_predictions'] = None
            try: result['qa_hybrid_score_vector'] = json.dumps(final_scores)
            except Exception: result['qa_hybrid_score_vector'] = None

            results.append(result)

        self.results_df = pd.DataFrame(results)

        # QA hesaplamaları
        dfq = self.results_df
        prob_cols = [c for c in dfq.columns if c.startswith('ENSEMBLE_PROB_')]
        if prob_cols:
            prob_mat = dfq[prob_cols].values
            max_prob = np.nanmax(prob_mat, axis=1)
            min_prob = np.nanmin(prob_mat, axis=1)
            sorted_probs = -np.sort(-prob_mat, axis=1)
            second_best = np.array([sorted_probs[i,1] if sorted_probs.shape[1] > 1 else 0.0 for i in range(sorted_probs.shape[0])])
            ratio_top_second = np.divide(max_prob, second_best, out=np.full_like(max_prob, np.inf), where=second_best>0)
            variance = np.nanvar(prob_mat, axis=1)
        else:
            prob_cols_rf = [c for c in dfq.columns if c.startswith('PROB_RF_')]
            if prob_cols_rf:
                prob_mat = dfq[prob_cols_rf].values
                max_prob = np.nanmax(prob_mat, axis=1)
                min_prob = np.nanmin(prob_mat, axis=1)
                sorted_probs = -np.sort(-prob_mat, axis=1)
                second_best = np.array([sorted_probs[i,1] if sorted_probs.shape[1] > 1 else 0.0 for i in range(sorted_probs.shape[0])])
                ratio_top_second = np.divide(max_prob, second_best, out=np.full_like(max_prob, np.inf), where=second_best>0)
                variance = np.nanvar(prob_mat, axis=1)
            else:
                prob_mat = None; max_prob = np.array([0.0]*len(dfq)); min_prob = np.array([0.0]*len(dfq))
                second_best = np.array([0.0]*len(dfq)); ratio_top_second = np.array([np.inf]*len(dfq)); variance = np.array([0.0]*len(dfq))

        has_pred_mask = dfq['RF_PRED'].notnull() & (dfq['RF_PRED'] != 'None')

        if has_pred_mask.sum() == 0:
            dfq['qa_confidence'] = np.nan; dfq['qa_min_prob'] = np.nan; dfq['qa_second_best_prob'] = np.nan
            dfq['qa_prob_ratio'] = np.nan; dfq['qa_variance'] = np.nan; dfq['qa_quartile_rank'] = np.nan
            dfq['qa_risk_level'] = np.nan; dfq['qa_flag_low_confidence'] = np.nan; dfq['qa_flag_high_risk'] = np.nan
            dfq['qa_flag_inconsistent'] = np.nan; dfq['qa_error_score'] = np.nan; dfq['qa_final_label'] = np.nan
        else:
            dfq.loc[has_pred_mask, 'qa_confidence'] = max_prob[has_pred_mask.values]
            dfq.loc[has_pred_mask, 'qa_min_prob'] = min_prob[has_pred_mask.values]
            dfq.loc[has_pred_mask, 'qa_second_best_prob'] = second_best[has_pred_mask.values]
            dfq.loc[has_pred_mask, 'qa_prob_ratio'] = np.where(np.isinf(ratio_top_second[has_pred_mask.values]), np.inf, ratio_top_second[has_pred_mask.values])
            dfq.loc[has_pred_mask, 'qa_variance'] = variance[has_pred_mask.values]
            confidences = dfq.loc[has_pred_mask, 'qa_confidence']
            quartiles = pd.qcut(confidences, 4, labels=False, duplicates='drop')
            dfq.loc[confidences.index, 'qa_quartile_rank'] = quartiles
            dfq['qa_quartile_rank'] = dfq['qa_quartile_rank'].fillna(0).astype(int)
            LOW_CONF_THRESHOLD = self.config.get('low_confidence_threshold', 0.6)
            HIGH_RISK_CONF_THRESHOLD = self.config.get('high_risk_confidence_threshold', 0.4)
            INCONSISTENCY_RATIO_THRESHOLD = self.config.get('inconsistency_ratio_threshold', 1.2)
            for i in dfq.index[has_pred_mask]:
                is_anomaly = bool(dfq.at[i, 'ANOMALY_FLAG'])
                conf = dfq.at[i, 'qa_confidence']
                if conf >= 0.8 and not is_anomaly: rl = 'low'
                elif conf >= 0.6 and not is_anomaly: rl = 'medium'
                else: rl = 'high'
                dfq.at[i, 'qa_risk_level'] = rl
                dfq.at[i, 'qa_flag_low_confidence'] = int(conf < LOW_CONF_THRESHOLD)
                dfq.at[i, 'qa_flag_high_risk'] = int((conf < HIGH_RISK_CONF_THRESHOLD) or is_anomaly)
                preds = [dfq.at[i, 'RF_PRED'], dfq.at[i, 'XGB_PRED'], dfq.at[i, 'LR_PRED']]
                preds_nonnull = [p for p in preds if p is not None]
                inconsistent = len(set(preds_nonnull)) > 1
                if inconsistent or (dfq.at[i, 'qa_prob_ratio'] < INCONSISTENCY_RATIO_THRESHOLD):
                    dfq.at[i, 'qa_flag_inconsistent'] = 1
                else:
                    dfq.at[i, 'qa_flag_inconsistent'] = 0
                dfq.at[i, 'qa_error_score'] = float(dfq.at[i, 'qa_flag_low_confidence'] * 1.0 + dfq.at[i, 'qa_flag_high_risk'] * 1.5 + dfq.at[i, 'qa_flag_inconsistent'] * 1.0)
                try:
                    best_col = max([c for c in dfq.columns if c.startswith('ENSEMBLE_PROB_')], key=lambda c: dfq.at[i, c])
                    dfq.at[i, 'qa_final_label'] = best_col.replace('ENSEMBLE_PROB_', '')
                except Exception:
                    dfq.at[i, 'qa_final_label'] = None
        dfq['qa_copq'] = np.nan
        for i in dfq.index:
            if pd.notna(dfq.at[i, 'qa_confidence']):
                label = dfq.at[i, 'qa_final_label']
                if isinstance(label, str):
                    lower = label.lower()
                    if 'karar-1' in lower or 'kabul' in lower or 'accept' in lower: lblnum = 0
                    else: lblnum = 1
                else: lblnum = 0
                cost_good = 1.0; cost_bad = 10.0; risk_mult = 2.0
                base_cost = cost_good if lblnum == 0 else cost_bad
                dfq.at[i, 'qa_copq'] = float(base_cost * (1 + risk_mult * (1 - float(dfq.at[i, 'qa_confidence']))))
            else:
                dfq.at[i, 'qa_copq'] = np.nan

        self.results_df = dfq
        return self

    # Export CSV with ALL information
    def export_full_csv(self, output_path='superset_quality_decisions_full.csv'):
        if self.results_df is None or self.results_df.empty:
            logger.warning("No results to export.")
            return None
        df = self.results_df.copy()
        for key, value in self.model_metrics.items():
            df[f'MODEL_METRIC_{key}'] = value
        for key, value in self.eda_summary.items():
            if isinstance(value, (int, float, str)):
                df[f'EDA_{key}'] = value
            elif isinstance(value, dict):
                df[f'EDA_{key}'] = json.dumps(value)
            elif isinstance(value, list):
                df[f'EDA_{key}'] = json.dumps(value)
        if self.feature_importance is not None:
            top_features = self.feature_importance.head(10)
            for idx, row in top_features.iterrows():
                col_name = f"FEATURE_IMP_{row['feature']}"
                imp_value = row.get('avg_importance', row.get('rf_importance', 0))
                df[col_name] = imp_value
        if self.qa_metrics_summary is not None:
            for idx, row in self.qa_metrics_summary.iterrows():
                method = row['Decision_Method']
                df[f'QA_ACCURACY_{method}'] = row['Accuracy']
                df[f'QA_F1_MACRO_{method}'] = row['F1_Macro']
        if 'qa_prob_vector' in df.columns:
            df['qa_prob_vector'] = df['qa_prob_vector'].astype(str)
        if 'qa_model_predictions' in df.columns:
            df['qa_model_predictions'] = df['qa_model_predictions'].astype(str)
        if 'qa_hybrid_score_vector' in df.columns:
            df['qa_hybrid_score_vector'] = df['qa_hybrid_score_vector'].astype(str)
        try:
            prob_cols_dynamic = []
            if 'TARGET_DECISION' in self.label_encoders:
                class_names = list(self.label_encoders['TARGET_DECISION'].classes_)
                for cls in class_names:
                    prob_cols_dynamic.extend([f'PROB_RF_{cls}', f'PROB_XGB_{cls}', f'PROB_LR_{cls}', f'ENSEMBLE_PROB_{cls}'])
            FINAL_COLUMNS_FOR_DASHBOARD = [
            'WORK_ORDER', 'ACTUAL_DECISION', 'RF_PRED', 'XGB_PRED', 'LR_PRED',
            'HYBRID_OPTIMIZATION_DECISION',
            'MODEL1_COPQ_RISK_SCORE',
            'MODEL2_CC_OVERLOAD_PENALTY',
            'MODEL3_BATCH_FLOW_FACTOR',
            'MODEL4_ASSIGNMENT_RISK_FACTOR',
            'OR_DECISION', 'DETERMINISTIC_DECISION', 'ROBUST_DECISION',
            ] + prob_cols_dynamic + [
            'qa_confidence', 'qa_copq', 'qa_error_score', 'qa_final_label',
            'qa_flag_high_risk', 'qa_flag_inconsistent', 'qa_min_prob',
            'qa_prob_ratio', 'qa_quartile_rank', 'qa_second_best_prob',
            'qa_variance', 'ANOMALY_FLAG', 'ANOMALY_SCORE', 'CLUSTER_ID',
            'CLUSTER_DISTANCE', 'DISTANCE_FROM_RESULT_MIDDLE_POINT', 'OUT_OF_LIMITS'
            ]
            cols_to_keep = [col for col in FINAL_COLUMNS_FOR_DASHBOARD if col in df.columns]
            df = df[cols_to_keep]
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logger.info("Exported full CSV to %s", output_path)
            print(f"\n Successfully exported comprehensive results to: {output_path}")
            print(f"   Total columns: {len(df.columns)}")
            print(f"   Total rows: {len(df)}")
            return output_path
        except Exception as e:
            logger.error("Failed to write CSV: %s", e)
            return None

    # Full pipeline wrapper
    def full_cycle(self, save_path='superset_quality_decisions_full.csv') -> Optional[str]:
        (self.load_and_prep_data()
             .perform_eda()
             .add_cost_center_features()
             .add_transition_features()
             .add_inefficiency_features()
             .add_pca_features()
             .add_mutual_info()
             .add_clusters()
             .add_anomaly_score()
             .add_corr_feature()
             .prepare_for_modeling()
             .train_rf_model()
             .train_xgboost_model()
             .train_logistic_model()
             .calculate_model_performance()
             .analyze_feature_importance()
             .process_all_data()
             .add_comprehensive_qa_metrics()
        )
        exported = self.export_full_csv(save_path)
        print(f"\n Summary Statistics:")
        print(f"   Total Records Processed: {len(self.results_df)}")
        print(f"   Labeled Records: {self.eda_summary.get('labeled_records', 'N/A')}")
        print(f"   Test Set Size: {self.model_metrics.get('test_set_size', 'N/A')}")
        print(f"   Train Set Size: {self.model_metrics.get('train_set_size', 'N/A')}")
        print(f"\n Models Trained:")
        print(f"   • Random Forest")
        print(f"   • XGBoost")
        print(f"   • Logistic Regression")
        print(f"\n Decision Strategies Implemented:")
        print(f"   • HYBRID OPTIMIZATION (Yeni: Model 1-4 Skorları ile oluşturulan)")
        print(f"   • Model 1 (COPQ Risk Skoru)")
        print(f"   • Model 2 (Cost Center Aşım Skoru)")
        print(f"   • Model 3 (Batch Flow Faktörü)")
        print(f"   • Model 4 (Anomaly/Inefficiency Risk Skoru)")
        print(f"   • OR (Maximum Probability - Baseline)")
        print(f"   • Deterministic (Cost-Quality-Capacity - Baseline)")
        print(f"   • Robust (Scenario-based - Baseline)")
        print(f"\n Analytical Techniques Applied:")
        print(f"   • PCA (Principal Component Analysis)")
        print(f"   • Mutual Information")
        print(f"   • K-Means Clustering")
        print(f"   • Anomaly Detection (Isolation Forest) - Cycle Time Anomaly Detection Added")
        print(f"   • Correlation Analysis")
        print(f"   • Feature Engineering")
        print(f"\n Performance Note:")
        print(f"   Test Set Accuracy vs Full Dataset Accuracy may differ significantly.")
        print(f"   Test set is a small random sample (~20%) which may have class imbalance.")
        print(f"   Full dataset metrics (in QA section) provide more stable estimates.")
        print(f"\n Output File: {exported}")
        print(f"   Contains ALL metrics, predictions, and analytical outputs in single CSV")

        return exported

if __name__ == '__main__':
    INPUT_FILE = 'QAISU - MODEL 1 EXAMPLE INPUT DATASET.xlsx'
    OUTPUT_CSV = 'QAISU - MODEL 1 EXAMPLE OUTPUT.csv'

    # Hibrit Karar Ağırlıkları Config'e Eklendi
    example_config = {
        'random_state': 42,
        'low_confidence_threshold': 0.6,
        'high_risk_confidence_threshold': 0.4,
        'inconsistency_ratio_threshold': 1.2,
        'inspection_type_weights': {
            'Gorsel': 1.0, 'Fonksiyonel': 1.5, 'Tahribatli': 2.0
        },
        'cost_center_capacity': {
            'MM01': 1000, 'MM02': 800, 'MM03': 1200
        },
        'overload_penalty': 2.0,
        'inspection_workload_hours': {
            'Gorsel': 0.5, 'Fonksiyonel': 1.5, 'Tahribatli': 3.0
        },
        'w_transition': 1.0,
        'w_flow': 1.5,
        'w_anomaly_penalty': 2.0,
        'w_inefficiency_penalty': 1.0,
        'inspection_unit_cost': 1.0,
        'defect_financial_weight': 10.0,
        'handling_costs': {
            'KABUL': 0.0, 'RTV': 1.0, 'RU': 2.0, 'ISLAH': 5.0, 'HURDA': 10.0,
        },
        'cost_vector': [1.0, 5.0, 10.0, 15.0, 20.0],
        'quality_vector': [1.0, 0.8, 0.6, 0.3, 0.9],
        'capacity_vector': [1.0, 0.9, 0.7, 0.5, 0.8],
        # HİBRİT KARAR AĞIRLIKLARI
        'W_COPQ': 1.5,
        'W_CC': 1.0,
        'W_FLOW': 1.5,
        'W_ASSIGN': 2.0,
    }

    print("""
    ╔══════════════════════════════════════════════════════════════════════════════╗
    ║                 QUALITY DECISION SYSTEM - COMPREHENSIVE                      ║
    ║                                                                              ║
    ║  Features:                                                                   ║
    ║  ✓ TARGET VARIABLE: 'NIHAI_KARAR' (Mapped to ACTUAL_DECISION)                ║
    ║  ✓ 3 ML Models (RF, XGBoost, Logistic Regression)                            ║
    ║  ✓ 6+ Analytical Techniques (PCA, MI, Clustering, Anomaly, etc.)             ║
    ║  ✓ 4 Optimization Models (Score Producers) & 1 HYBRID DECISION MODEL         ║
    ║  ✓ 3 Baseline Optimization Models (OR, Deterministic, Robust)                ║
    ║  ✓ Feature Importance Analysis                                               ║
    ║  ✓ Confusion Matrix & Per-Class Metrics                                      ║
    ║  ✓ Comprehensive QA Metrics                                                  ║
    ║  ✓ Single CSV Output with all information                                    ║
    ╚══════════════════════════════════════════════════════════════════════════════╝
    """)

    system = QualityDecisionSystem(INPUT_FILE, config=example_config)
    out = system.full_cycle(save_path=OUTPUT_CSV)